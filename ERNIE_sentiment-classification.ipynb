{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 千言情感分析比赛  - Gavin\n",
    "本篇是深度学习的自然语言处理课程的大作业，基于paddle高级api的千言情感分析比赛。\n",
    "\n",
    "该比赛分为三种级别情感分析：句子级情感分类、评价对象级情感分类、观点抽取。下面就三种情况进行建模。\n",
    "\n",
    "相关介绍以及比赛可以参考：\n",
    "\n",
    "[比赛和数据介绍](https://aistudio.baidu.com/aistudio/competition/detail/50)\n",
    "\n",
    "[具体原理与分析](https://aistudio.baidu.com/aistudio/projectdetail/1968542)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/05da0e36ac2945e58b9a0733cf68e4d86cf58d1a56c148ceb2e2a7259ef31930)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/7a/e6098c8794d7753470071f58b07843824c40ddbabe213eae458d321d2dbe/paddlenlp-2.0.3-py3-none-any.whl (451kB)\n",
      "     |████████████████████████████████| 460kB 762kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.1\n",
      "    Uninstalling paddlenlp-2.0.1:\n",
      "      Successfully uninstalled paddlenlp-2.0.1\n",
      "Successfully installed paddlenlp-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple \n",
    "!unzip -oq /home/aistudio/data/data94266/nlp_dataset.zip -d /home/aistudio/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 句子级情感分类\n",
    "| 数据集名称 | 训练集大小 | 开发集大小 | 测试集大小\n",
    "| -------- | -------- | -------- | -------- | \n",
    "| ChnSentiCorp     | 9,600     |1,200\t\t\t|1,200\n",
    "|NLPCC14-SC \t |10,000 \t |/ \t |2,500\n",
    "\n",
    "```\n",
    "ChnSentiCorp\n",
    "train:\n",
    "label\ttext_a\n",
    "1\t选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。\n",
    "\n",
    "test:\n",
    "qid\ttext_a\n",
    "0\t这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般\n",
    "\n",
    "dev:\n",
    "qid\tlabel\ttext_a\n",
    "0\t1\t這間酒店環境和服務態度亦算不錯,但房間空間太小~~不宣容納太大件行李~~且房間格調還可以~~ \n",
    "```\n",
    "\n",
    "```\n",
    "NLPCC14-SC\n",
    "train:\n",
    "label\ttext_a\n",
    "1\t请问这机不是有个遥控器的吗？\n",
    "\n",
    "test:\n",
    "qid\ttext_a\n",
    "0\t我终于找到同道中人啦～～～～从初中开始，我就已经喜欢上了michaeljackson.但同学们都用鄙夷的眼光看我。。。。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '1', 'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般'}\n",
      "{'qid': '0', 'text': '我终于找到同道中人啦～～～～从初中开始，我就已经喜欢上了michaeljackson.但同学们都用鄙夷的眼光看我，他们人为jackson的样子古怪甚至说＂丑＂．我当场气晕．但现在有同道中人了，我好开心！！！michaeljacksonisthemostsuccessfulsingerintheworld!!~~~'}\n"
     ]
    }
   ],
   "source": [
    "## 加载数据，预处理\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "def read(data_path, data_type='train'):\r\n",
    "    if data_type=='train':\r\n",
    "        with open(data_path, 'r', encoding='utf8') as f:\r\n",
    "            for line in f.readlines()[1:]:\r\n",
    "                label, text = line.strip().split('\\t')\r\n",
    "                yield {'label': label, 'text': text}\r\n",
    "    elif data_type=='dev':\r\n",
    "        with open(data_path, 'r', encoding='utf8') as f:\r\n",
    "            for line in f.readlines()[1:]:\r\n",
    "                qid, label, text = line.strip().split('\\t')\r\n",
    "                yield {'qid': qid, 'label': label, 'text': text}\r\n",
    "    else:\r\n",
    "        with open(data_path, 'r', encoding='utf8') as f:\r\n",
    "            for line in f.readlines()[1:]:\r\n",
    "                qid, text = line.strip().split('\\t')\r\n",
    "                yield {'qid': qid, 'text': text}\r\n",
    "\r\n",
    "# 加载两个数据集的数据\r\n",
    "data_dict = {'ChnSentiCorp':{'test': load_dataset(read, data_path='dataset/ChnSentiCorp/test.tsv', data_type='test', lazy=False),\r\n",
    "                             'train': load_dataset(read, data_path='dataset/ChnSentiCorp/train.tsv', data_type='train', lazy=False),\r\n",
    "                             'dev': load_dataset(read, data_path='dataset/ChnSentiCorp/dev.tsv', data_type='dev', lazy=False)},\r\n",
    "            'NLPCC14-SC': {'test': load_dataset(read, data_path='dataset/NLPCC14-SC/test.tsv', data_type='test', lazy=False),\r\n",
    "                           'train': load_dataset(read, data_path='dataset/NLPCC14-SC/train.tsv', data_type='train', lazy=False)}\r\n",
    "                           }\r\n",
    "print(data_dict['ChnSentiCorp']['train'][0])\r\n",
    "print(data_dict['NLPCC14-SC']['test'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.2 构造数据Dataloader（句子级）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 借鉴 【NLP打卡营】\r\n",
    "from functools import partial\r\n",
    "import paddle\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "from paddle.io import DataLoader\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def convert_example(example,\r\n",
    "                    tokenizer,\r\n",
    "                    max_seq_length=512,\r\n",
    "                    is_test=False):\r\n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\r\n",
    "    encoded_inputs = tokenizer(\r\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\r\n",
    "\r\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\r\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        # label：情感极性类别\r\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, label\r\n",
    "    else:\r\n",
    "        # qid：每条数据的编号\r\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, qid\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data_set, tokenizer, batch_size=32, max_seq_length=128, for_test=False):\r\n",
    "    # 将数据处理成模型可读入的数据格式\r\n",
    "    trans_func = partial(\r\n",
    "        convert_example,\r\n",
    "        tokenizer=tokenizer,\r\n",
    "        max_seq_length=max_seq_length,\r\n",
    "        is_test=for_test)\r\n",
    "\r\n",
    "    data_set = data_set.map(trans_func)\r\n",
    "    \r\n",
    "    # 将数据组成批量式数据，如\r\n",
    "    # 将不同长度的文本序列padding到批量式数据中最大长度\r\n",
    "    # 将每条数据label堆叠在一起\r\n",
    "    batchify_fn = lambda samples, fn=Tuple(\r\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "        Stack()  # labels\r\n",
    "    ): [data for data in fn(samples)]\r\n",
    "\r\n",
    "    \r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    if for_test:\r\n",
    "        sampler = paddle.io.BatchSampler(\r\n",
    "            dataset=data_set, batch_size=batch_size, shuffle=shuffle)\r\n",
    "    else:\r\n",
    "        sampler = paddle.io.DistributedBatchSampler(\r\n",
    "            dataset=data_set, batch_size=batch_size, shuffle=shuffle)\r\n",
    "\r\n",
    "    data_loader = DataLoader(dataset=data_set, batch_sampler=sampler, collate_fn=batchify_fn)\r\n",
    "\r\n",
    "    return data_loader\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.3 搭建模型（句子级）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-22 17:44:34,642] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams and saved to /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch\n",
      "[2021-06-22 17:44:34,645] [    INFO] - Downloading skep_ernie_1.0_large_ch.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams\n",
      "100%|██████████| 1238309/1238309 [00:18<00:00, 65632.67it/s]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-22 17:45:26,023] [    INFO] - Downloading skep_ernie_1.0_large_ch.vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.vocab.txt\n",
      "100%|██████████| 55/55 [00:00<00:00, 26289.08it/s]\n"
     ]
    }
   ],
   "source": [
    "## 加载预训练模型\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\r\n",
    "\r\n",
    "## 加载模型，数据标签只有 2种，0 、 1\r\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=2)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")\r\n",
    "\r\n",
    "@paddle.no_grad()\r\n",
    "def evaluate(model, criterion, metric, data_loader):\r\n",
    "    \"\"\"\r\n",
    "    Given a dataset, it evals model and computes the metric.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\r\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\r\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\r\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\r\n",
    "    \"\"\"\r\n",
    "    model.eval()\r\n",
    "    metric.reset()\r\n",
    "    losses = []\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        losses.append(loss.numpy())\r\n",
    "        correct = metric.compute(logits, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        accu = metric.accumulate()\r\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\r\n",
    "    model.train()\r\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.3 训练模型（句子级）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import time\r\n",
    "import paddle.nn.functional as F\r\n",
    "# 参数\r\n",
    "batch_size = 64   # 批量数据大小\r\n",
    "max_seq_length = 128  # 文本序列最大长度\r\n",
    "epochs = 1\r\n",
    "learning_rate = 2e-5\r\n",
    "# 训练过程中保存模型参数的文件夹\r\n",
    "ckpt_dir = \"model/sentc\"\r\n",
    "\r\n",
    "## 优化\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=learning_rate,\r\n",
    "    parameters=model.parameters())   # Adam优化器\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()     # 交叉熵损失函数\r\n",
    "metric = paddle.metric.Accuracy()  # accuracy评价指标\r\n",
    "\r\n",
    "# 数据\r\n",
    "data_name = 'NLPCC14-SC'     # ChnSentiCorp      NLPCC14-SC\r\n",
    "# print(data_dict[data_name]['train'][0])\r\n",
    "## 数据相关\r\n",
    "train_data_loader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_seq_length, for_test=False)\r\n",
    "if data_name == 'ChnSentiCorp':\r\n",
    "    dev_data_loader = get_data_loader(data_dict[data_name]['dev'], tokenizer, batch_size, max_seq_length, for_test=False)\r\n",
    "else:\r\n",
    "    dev_data_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 9, loss: 0.68500, accu: 0.58906, speed: 0.78 step/s\n",
      "global step 20, epoch: 1, batch: 19, loss: 0.48907, accu: 0.66172, speed: 0.35 step/s\n",
      "global step 30, epoch: 1, batch: 29, loss: 0.32096, accu: 0.70469, speed: 0.31 step/s\n",
      "global step 40, epoch: 1, batch: 39, loss: 0.47738, accu: 0.73242, speed: 0.34 step/s\n",
      "global step 50, epoch: 1, batch: 49, loss: 0.58489, accu: 0.75125, speed: 0.34 step/s\n",
      "global step 60, epoch: 1, batch: 59, loss: 0.50317, accu: 0.75781, speed: 0.34 step/s\n",
      "global step 70, epoch: 1, batch: 69, loss: 0.35701, accu: 0.76562, speed: 0.34 step/s\n",
      "global step 80, epoch: 1, batch: 79, loss: 0.42373, accu: 0.77070, speed: 0.34 step/s\n",
      "global step 90, epoch: 1, batch: 89, loss: 0.45871, accu: 0.77587, speed: 0.35 step/s\n",
      "global step 100, epoch: 1, batch: 99, loss: 0.38557, accu: 0.77750, speed: 0.34 step/s\n",
      "global step 110, epoch: 1, batch: 109, loss: 0.47060, accu: 0.78068, speed: 0.34 step/s\n",
      "global step 120, epoch: 1, batch: 119, loss: 0.39330, accu: 0.78542, speed: 0.34 step/s\n",
      "global step 130, epoch: 1, batch: 129, loss: 0.38429, accu: 0.78846, speed: 0.34 step/s\n",
      "global step 140, epoch: 1, batch: 139, loss: 0.44955, accu: 0.79141, speed: 0.34 step/s\n",
      "global step 150, epoch: 1, batch: 149, loss: 0.41711, accu: 0.79500, speed: 0.34 step/s\n"
     ]
    }
   ],
   "source": [
    "## 训练\r\n",
    "# 开启训练\r\n",
    "global_step = 0\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader):\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "        # 喂数据给model\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        # 计算损失函数值\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        # 预测分类概率值\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        # 计算acc\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0:\r\n",
    "            print(\r\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss, acc,\r\n",
    "                    10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "        \r\n",
    "        # 反向梯度回传，更新参数\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % 10 == 0:\r\n",
    "            save_dir = os.path.join(ckpt_dir, data_name)\r\n",
    "            if not os.path.exists(save_dir):\r\n",
    "                os.makedirs(save_dir)\r\n",
    "            # 评估当前训练的模型\r\n",
    "            if dev_data_loader:\r\n",
    "                evaluate(model, criterion, metric, dev_data_loader)\r\n",
    "            # 保存当前模型参数等\r\n",
    "            model.save_pretrained(save_dir)\r\n",
    "            # 保存tokenizer的词表等\r\n",
    "            tokenizer.save_pretrained(save_dir)\r\n",
    "print('finish train!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.4 预测并保存结果（句子级）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from model/sentc/NLPCC14-SC/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "# data_name =  'ChnSentiCorp'     # ChnSentiCorp      NLPCC14-SC\r\n",
    "test_data_loader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_seq_length, for_test=True)\r\n",
    "\r\n",
    "\r\n",
    "# 根据实际运行情况，更换加载的参数路径\r\n",
    "params_path = os.path.join(ckpt_dir, data_name + '/model_state.pdparams')\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    # 加载模型参数\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)\r\n",
    "\r\n",
    "label_map = {0: '0', 1: '1'}\r\n",
    "results = []\r\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\r\n",
    "model.eval()\r\n",
    "for batch in test_data_loader:\r\n",
    "    input_ids, token_type_ids, qids = batch\r\n",
    "    # 喂数据给模型\r\n",
    "    logits = model(input_ids, token_type_ids)\r\n",
    "    # 预测分类\r\n",
    "    probs = F.softmax(logits, axis=-1)\r\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "    idx = idx.tolist()\r\n",
    "    labels = [label_map[i] for i in idx]\r\n",
    "    qids = qids.numpy().tolist()\r\n",
    "    results.extend(zip(qids, labels))\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "save_dir = {'ChnSentiCorp': './submission/ChnSentiCorp.tsv', 'NLPCC14-SC': './submission/NLPCC14-SC.tsv'}\r\n",
    "res_dir = save_dir[data_name]\r\n",
    "if not os.path.exists('./submission'):\r\n",
    "    os.makedirs('./submission')\r\n",
    "# 写入预测结果\r\n",
    "with open(res_dir, 'w', encoding=\"utf8\") as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for qid, label in results:\r\n",
    "        f.write(str(qid[0])+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2 评价对象级情感分类\n",
    "\n",
    "| 数据集名称 | 训练集大小 | 开发集大小 | 测试集大小\n",
    "| -------- | -------- | -------- | -------- | \n",
    "| SE-ABSA16_PHNS     | 1,336    |/\t\t\t|529\n",
    "|SE-ABSA16_CAME\t |1,317\t |/ \t |505\n",
    "\n",
    "\n",
    "```\n",
    "SE-ABSA16_PHNS\n",
    "train:\n",
    "label\ttext_a\ttext_b\n",
    "1\tphone#design_features\t今天有幸拿到了港版白色iPhone 5真机，试玩了一下，说说感受吧：1. 真机尺寸宽度与4/4s保持一致没有变化，长度多了大概一厘米，也就是之前所说的多了一排的图标。2. 真机重量比上一代轻了很多，个人感觉跟i9100的重量差不多。（用惯上一代的朋友可能需要一段时间适应了）3. 由于目前还没有版的SIM卡，无法插卡使用，有购买的朋友要注意了，并非简单的剪卡就可以用，而是需要去运营商更换新一代的SIM卡。4. 屏幕显示效果确实比上一代有进步，不论是从清晰度还是不同角度的视角，iPhone 5绝对要更上一层，我想这也许是相对上一代最有意义的升级了。5. 新的数据接口更小，比上一代更好用更方便，使用的过程会有这样的体会。6. 从简单的几个操作来讲速度比4s要快，这个不用测试软件也能感受出来，比如程序的调用以及照片的拍摄和浏览。不过，目前水货市场上坑爹的价格，最好大家可以再观望一下，不要急着出手。\n",
    "\n",
    "test:\n",
    "qid\ttext_a\ttext_b\n",
    "0\tsoftware#usability\t刚刚入手8600，体会。刚刚从淘宝购买，1635元（包邮）。1、全新，应该是欧版机，配件也是正品全新。2、在三星官网下载了KIES，可用免费软件非常多，绝对够用。3、不到2000元能买到此种手机，知足了。\n",
    "```\n",
    "\n",
    "```\n",
    "SE-ABSA16_CAME\n",
    "train:\n",
    "label\ttext_a\ttext_b\n",
    "0\tcamera#design_features\t千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。\n",
    "\n",
    "test:\n",
    "qid\ttext_a\ttext_b\n",
    "0\tcamera#quality\t一直潜水，昨天入d300s +35 1.8g，谈谈感受，dx说，标题一定要长！在我们这尼康一个代理商开的大型体验中心提的货，老板和销售mm都很热情，不欺诈，也没有店大欺客，mm很热情，从d300s到d800，d7000，到d3x配各种镜头，全部把玩了一番，感叹啊，真他妈好东西！尤其d3x，有钱了，一定要他妈买一个，还有，就是d800，一摸心中的神机，顿时凉了半截，可能摸她之前，摸了她们的头牌，d3x的缘故，这手感 真是差了点，样子嘛，之所以喜欢尼康，就是喜欢棱角分明的感觉，d3x方方正正 ，甚是讨喜，d800这丫头，变得圆滑了不少，不喜欢。都说电子产品，买新不买旧，我倒不认为这么看，中低端产品的确如此，但顶级的高端产品，真不是这么回事啊，d3x也是51点对焦，我的d300s也是51点，但明显感觉，对焦就是比d300s 快，准，暗部反差较小时，也很少拉风箱，我的d300s就不行，光线不好反差较小，拉回来拉过去，半天合不上焦，说真的，一分价钱一分货啊，d800电子性能 肯定是先进的，但机械性能 跟d3x还是没可比性，传感器固然先进，但三千多万 像素 和两千多万像素 对我们来说，真的差别这么大吗？d800e3万多，有这钱真的不如加点买 d3x啊，真要是d3x烂，为什么尼康不停产了？人说高像素 是给商业摄影师用，我们的音乐老师，是业余的音乐制作人，也拍摄一些商业广告，平时他玩的时候 都是数码什么的，nc 加起来十几个，大三元全都配齐，但干活的时候，还是120的机器，照他那话说，数码 像素太低，不够用啊！废话说太多了，谈谈感受吧，当初一直在纠结d7000和d300s，都说什么d7000画质超越d300s，我也信，但昨天拿到实机后，我瞬间就决定 d300s了，我的手算小的，握住d300s，我感觉，刚刚好，而且手柄凹槽 ，我觉得还不够深，握感不是十分的充盈，这点要像宾得k5学习，而且d7000小了一点，背部操作空间局促，大拇指没地放，果断d300s，而且试机的时候，我给d300s 换上了24-70，可能我练健身比较久了，没感觉有啥重量，蛮趁手的，现在配35 1.8 感觉轻飘飘的，哈哈，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1 数据提取预处理（评价对象级）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "import requests\r\n",
    "import re\r\n",
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "num=392\r\n",
    "def translator(str):\r\n",
    "    \"\"\"\r\n",
    "    input : str 需要翻译的字符串\r\n",
    "    output：translation 翻译后的字符串\r\n",
    "    有每小时1000次访问的限制\r\n",
    "    \"\"\"\r\n",
    "    global  num;\r\n",
    "    num=num+1\r\n",
    "    print(\"Program has process %d times \"%num)\r\n",
    "    # API\r\n",
    "    url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&smartresult=ugc&sessionFrom=null'\r\n",
    "    # 传输的参数， i为要翻译的内容\r\n",
    "    key = {\r\n",
    "        'type': \"AUTO\",\r\n",
    "        'i': str,\r\n",
    "        \"doctype\": \"json\",\r\n",
    "        \"version\": \"2.1\",\r\n",
    "        \"keyfrom\": \"fanyi.web\",\r\n",
    "        \"ue\": \"UTF-8\",\r\n",
    "        \"action\": \"FY_BY_CLICKBUTTON\",\r\n",
    "        \"typoResult\": \"true\"\r\n",
    "    }\r\n",
    "    # key 这个字典为发送给有道词典服务器的内容\r\n",
    "    response = requests.post(url, data=key)\r\n",
    "    # 判断服务器是否相应成功\r\n",
    "    if response.status_code == 200:\r\n",
    "        # 通过 json.loads 把返回的结果加载成 json 格式\r\n",
    "        result = json.loads(response.text)\r\n",
    "#         print (\"输入的词为：%s\" % result['translateResult'][0][0]['src'])\r\n",
    "#         print (\"翻译结果为：%s\" % result['translateResult'][0][0]['tgt'])\r\n",
    "        translation = result['translateResult'][0][0]['tgt']\r\n",
    "        return translation\r\n",
    "    else:\r\n",
    "        print(\"有道词典调用失败\")\r\n",
    "        # 相应失败就返回空\r\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tran_dict = {}\r\n",
    "# def tranlation_text(texts):\r\n",
    "#     if texts in tran_dict:\r\n",
    "#         return tran_dict[texts]\r\n",
    "#     else:\r\n",
    "#         texts_list = texts.strip().split('#')\r\n",
    "#         texts_list = [e.replace('_', ' ') for e in texts_list]\r\n",
    "#         texts_chs_list = [translator(e) for e in texts_list]\r\n",
    "#         texts_chs = '#'.join(texts_chs_list)\r\n",
    "#         tran_dict[texts] = texts_chs\r\n",
    "#     return texts_chs\r\n",
    "\r\n",
    "# print(tranlation_text('phone#design_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '0', 'text': '相机#设计特点', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。'}, {'label': '0', 'text': '相机#操作性能', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。'}, {'label': '0', 'text': '硬件#可用性', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。'}, {'label': '0', 'text': '软件#设计特点', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。'}, {'label': '1', 'text': '镜头#一般', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。'}]\n",
      "{'qid': '0', 'text': '相机#质量', 'text_pair': '一直潜水，昨天入d300s +35 1.8g，谈谈感受，dx说，标题一定要长！在我们这尼康一个代理商开的大型体验中心提的货，老板和销售mm都很热情，不欺诈，也没有店大欺客，mm很热情，从d300s到d800，d7000，到d3x配各种镜头，全部把玩了一番，感叹啊，真他妈好东西！尤其d3x，有钱了，一定要他妈买一个，还有，就是d800，一摸心中的神机，顿时凉了半截，可能摸她之前，摸了她们的头牌，d3x的缘故，这手感 真是差了点，样子嘛，之所以喜欢尼康，就是喜欢棱角分明的感觉，d3x方方正正 ，甚是讨喜，d800这丫头，变得圆滑了不少，不喜欢。都说电子产品，买新不买旧，我倒不认为这么看，中低端产品的确如此，但顶级的高端产品，真不是这么回事啊，d3x也是51点对焦，我的d300s也是51点，但明显感觉，对焦就是比d300s 快，准，暗部反差较小时，也很少拉风箱，我的d300s就不行，光线不好反差较小，拉回来拉过去，半天合不上焦，说真的，一分价钱一分货啊，d800电子性能 肯定是先进的，但机械性能 跟d3x还是没可比性，传感器固然先进，但三千多万 像素 和两千多万像素 对我们来说，真的差别这么大吗？d800e3万多，有这钱真的不如加点买 d3x啊，真要是d3x烂，为什么尼康不停产了？人说高像素 是给商业摄影师用，我们的音乐老师，是业余的音乐制作人，也拍摄一些商业广告，平时他玩的时候 都是数码什么的，nc 加起来十几个，大三元全都配齐，但干活的时候，还是120的机器，照他那话说，数码 像素太低，不够用啊！废话说太多了，谈谈感受吧，当初一直在纠结d7000和d300s，都说什么d7000画质超越d300s，我也信，但昨天拿到实机后，我瞬间就决定 d300s了，我的手算小的，握住d300s，我感觉，刚刚好，而且手柄凹槽 ，我觉得还不够深，握感不是十分的充盈，这点要像宾得k5学习，而且d7000小了一点，背部操作空间局促，大拇指没地放，果断d300s，而且试机的时候，我给d300s 换上了24-70，可能我练健身比较久了，没感觉有啥重量，蛮趁手的，现在配35 1.8 感觉轻飘飘的，哈哈，'}\n"
     ]
    }
   ],
   "source": [
    "## 加载数据，预处理\r\n",
    "import os\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "def read(data_path, data_type='train'):\r\n",
    "    if data_type=='train':\r\n",
    "        with open(data_path, 'r', encoding='utf8') as f:\r\n",
    "            for line in f.readlines()[1:]:\r\n",
    "                label, text, text_pair = line.strip().split('\\t')\r\n",
    "                # text = tranlation_text(text)\r\n",
    "                yield {'label': label, 'text': text, 'text_pair': text_pair}\r\n",
    "    else:\r\n",
    "        with open(data_path, 'r', encoding='utf8') as f:\r\n",
    "            for line in f.readlines()[1:]:\r\n",
    "                qid, text, text_pair = line.strip().split('\\t')\r\n",
    "                # text = tranlation_text(text)\r\n",
    "                yield {'qid': qid, 'text': text, 'text_pair': text_pair}\r\n",
    "\r\n",
    "# 加载两个数据集的数据\r\n",
    "data_dict = {'SE-ABSA16_PHNS':{'test': load_dataset(read, data_path='dataset/SE-ABSA16_PHNS/test.tsv', data_type='test', lazy=False),\r\n",
    "                             'train': load_dataset(read, data_path='dataset/SE-ABSA16_PHNS/train.tsv', data_type='train', lazy=False)},\r\n",
    "            'SE-ABSA16_CAME': {'test': load_dataset(read, data_path='dataset/SE-ABSA16_CAME/test.tsv', data_type='test', lazy=False),\r\n",
    "                           'train': load_dataset(read, data_path='dataset/SE-ABSA16_CAME/train.tsv', data_type='train', lazy=False)}\r\n",
    "                           }\r\n",
    "print(data_dict['SE-ABSA16_CAME']['train'][0:5])\r\n",
    "print(data_dict['SE-ABSA16_CAME']['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for name in data_dict.keys():\r\n",
    "#     if not os.path.exists(name):\r\n",
    "#         os.makedirs(name)\r\n",
    "#     for part, a in {'train':'label', 'test':'qid'}.items():\r\n",
    "#         res_dir = part + '.tsv'\r\n",
    "#         res_dir = os.path.join(name, res_dir)\r\n",
    "#         with open(res_dir, 'w', encoding=\"utf8\") as f:\r\n",
    "#             f.write(a+\"\\ttext_a\\ttext_b\\n\")\r\n",
    "#             for idx, text in enumerate(data_dict[name][part]):\r\n",
    "#                 f.write('\\t'.join(text.values())+\"\\n\")\r\n",
    "# print('fininsh save ch_text!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 准备DataLoader（评价对象级）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 借鉴 【NLP打卡营】\r\n",
    "from functools import partial\r\n",
    "import paddle\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "from paddle.io import DataLoader\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def convert_example(example,\r\n",
    "                    tokenizer,\r\n",
    "                    max_seq_length=512,\r\n",
    "                    is_test=False,\r\n",
    "                    dataset_name=\"SE-ABSA16_PHNS\"):\r\n",
    "    encoded_inputs = tokenizer(\r\n",
    "        text=example[\"text\"],\r\n",
    "        text_pair=example[\"text_pair\"],\r\n",
    "        max_seq_len=max_seq_length)\r\n",
    "\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, label\r\n",
    "    else:\r\n",
    "        return input_ids, token_type_ids\r\n",
    "\r\n",
    "def get_data_loader(data_set, tokenizer, batch_size=32, max_seq_length=128, for_test=False):\r\n",
    "    # 将数据处理成模型可读入的数据格式\r\n",
    "    trans_func = partial(\r\n",
    "        convert_example,\r\n",
    "        tokenizer=tokenizer,\r\n",
    "        max_seq_length=max_seq_length,\r\n",
    "        is_test=for_test)\r\n",
    "    \r\n",
    "    if for_test:\r\n",
    "        batchify_fn = lambda samples, fn=Tuple(\r\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "        ): [data for data in fn(samples)]\r\n",
    "    else:\r\n",
    "        batchify_fn = lambda samples, fn=Tuple(\r\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "            Stack(dtype=\"int64\")  # labels\r\n",
    "        ): [data for data in fn(samples)]\r\n",
    "\r\n",
    "    data_set = data_set.map(trans_func)    \r\n",
    "        \r\n",
    "    # 将数据组成批量式数据，如\r\n",
    "    # 将不同长度的文本序列padding到批量式数据中最大长度\r\n",
    "    # 将每条数据label堆叠在一起\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    if for_test:\r\n",
    "        sampler = paddle.io.BatchSampler(\r\n",
    "            dataset=data_set, batch_size=batch_size, shuffle=shuffle)\r\n",
    "    else:\r\n",
    "        sampler = paddle.io.DistributedBatchSampler(\r\n",
    "            dataset=data_set, batch_size=batch_size, shuffle=shuffle)\r\n",
    "\r\n",
    "    data_loader = DataLoader(dataset=data_set, batch_sampler=sampler, collate_fn=batchify_fn)\r\n",
    "\r\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 11:17:30,325] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-24 11:17:41,347] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "## 加载预训练模型\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\r\n",
    "## 加载模型，数据标签只有 2种，0 、 1\r\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=2)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "@paddle.no_grad()\r\n",
    "def predict(model, data_loader, label_map):\r\n",
    "    \"\"\"\r\n",
    "    Given a prediction dataset, it gives the prediction results.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\r\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\r\n",
    "        label_map(obj:`dict`): The label id (key) to label str (value) map.\r\n",
    "    \"\"\"\r\n",
    "    model.eval()\r\n",
    "    results = []\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, token_type_ids = batch\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "        idx = idx.tolist()\r\n",
    "        labels = [label_map[i] for i in idx]\r\n",
    "        results.extend(labels)\r\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3 训练评价对象级模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import time\r\n",
    "import paddle.nn.functional as F\r\n",
    "# 参数\r\n",
    "batch_size = 16   # 批量数据大小\r\n",
    "max_seq_length = 512  # 文本序列最大长度\r\n",
    "epochs = 3\r\n",
    "learning_rate = 2e-5\r\n",
    "# 训练过程中保存模型参数的文件夹\r\n",
    "ckpt_dir = \"model/tager\"\r\n",
    "\r\n",
    "paraparam_ = {'SE-ABSA16_PHNS': [{'batch_size': 8, 'max_seq_length':512, 'epochs':3, 'learning_rate': 2e-5},\r\n",
    "                                 {'batch_size': 16, 'max_seq_length':512, 'epochs':3, 'learning_rate': 2e-5}]\r\n",
    "                                 }\r\n",
    "\r\n",
    "## 优化\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=learning_rate,\r\n",
    "    parameters=model.parameters())   # Adam优化器\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()     # 交叉熵损失函数\r\n",
    "metric = paddle.metric.Accuracy()  # accuracy评价指标\r\n",
    "\r\n",
    "# 数据\r\n",
    "data_name = 'SE-ABSA16_CAME'     # SE-ABSA16_PHNS     SE-ABSA16_CAME\r\n",
    "# print(data_dict[data_name]['train'][0])\r\n",
    "## 数据相关\r\n",
    "train_data_loader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_seq_length, for_test=False)\r\n",
    "dev_data_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.59546, acc: 0.61875, speed: 0.64 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.63994, acc: 0.58437, speed: 0.62 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.67472, acc: 0.60000, speed: 0.63 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.60166, acc: 0.60781, speed: 0.63 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.84503, acc: 0.61000, speed: 0.62 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.55087, acc: 0.61979, speed: 0.32 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.46888, acc: 0.63214, speed: 0.62 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.65108, acc: 0.63906, speed: 0.63 step/s\n",
      "global step 90, epoch: 2, batch: 7, loss: 0.61885, acc: 0.64311, speed: 0.65 step/s\n",
      "global step 100, epoch: 2, batch: 17, loss: 0.46846, acc: 0.65324, speed: 0.62 step/s\n",
      "global step 110, epoch: 2, batch: 27, loss: 0.58769, acc: 0.65866, speed: 0.31 step/s\n",
      "global step 120, epoch: 2, batch: 37, loss: 0.52489, acc: 0.65898, speed: 0.62 step/s\n",
      "global step 130, epoch: 2, batch: 47, loss: 0.79648, acc: 0.66216, speed: 0.62 step/s\n",
      "global step 140, epoch: 2, batch: 57, loss: 0.65285, acc: 0.66712, speed: 0.62 step/s\n",
      "global step 150, epoch: 2, batch: 67, loss: 0.68010, acc: 0.66890, speed: 0.62 step/s\n",
      "global step 160, epoch: 2, batch: 77, loss: 0.64384, acc: 0.67203, speed: 0.31 step/s\n",
      "global step 170, epoch: 3, batch: 4, loss: 0.51168, acc: 0.67494, speed: 0.66 step/s\n",
      "global step 180, epoch: 3, batch: 14, loss: 0.46609, acc: 0.68160, speed: 0.62 step/s\n",
      "global step 190, epoch: 3, batch: 24, loss: 0.71054, acc: 0.68489, speed: 0.62 step/s\n",
      "global step 200, epoch: 3, batch: 34, loss: 0.46168, acc: 0.68660, speed: 0.61 step/s\n",
      "global step 210, epoch: 3, batch: 44, loss: 0.66543, acc: 0.68634, speed: 0.31 step/s\n",
      "global step 220, epoch: 3, batch: 54, loss: 0.47784, acc: 0.68639, speed: 0.63 step/s\n",
      "global step 230, epoch: 3, batch: 64, loss: 0.82583, acc: 0.68917, speed: 0.62 step/s\n",
      "global step 240, epoch: 3, batch: 74, loss: 0.81522, acc: 0.68937, speed: 0.62 step/s\n",
      "global step 250, epoch: 4, batch: 1, loss: 0.48789, acc: 0.69019, speed: 0.66 step/s\n",
      "global step 260, epoch: 4, batch: 11, loss: 0.49974, acc: 0.69227, speed: 0.31 step/s\n",
      "global step 270, epoch: 4, batch: 21, loss: 0.48159, acc: 0.69349, speed: 0.62 step/s\n",
      "global step 280, epoch: 4, batch: 31, loss: 0.51038, acc: 0.69373, speed: 0.62 step/s\n",
      "global step 290, epoch: 4, batch: 41, loss: 0.34231, acc: 0.69568, speed: 0.62 step/s\n",
      "global step 300, epoch: 4, batch: 51, loss: 0.66003, acc: 0.69645, speed: 0.62 step/s\n",
      "global step 310, epoch: 4, batch: 61, loss: 0.21303, acc: 0.70002, speed: 0.31 step/s\n",
      "global step 320, epoch: 4, batch: 71, loss: 0.33104, acc: 0.70277, speed: 0.62 step/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-54ee87fb7f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# 计算损失函数值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# 预测分类概率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, label)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0muse_softmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_softmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             name=self.name)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/functional/loss.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, label, weight, ignore_index, reduction, soft_label, axis, use_softmax, name)\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'soft_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoft_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore_index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m             \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'numeric_stable_mode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             'use_softmax', use_softmax)\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "        # 喂数据给model\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        # 计算损失函数值\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        # 预测分类概率\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        # 计算acc\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0:\r\n",
    "            print(\r\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss, acc,\r\n",
    "                    10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "        \r\n",
    "        # 反向梯度回传，更新参数\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % 50 == 0:\r\n",
    "            save_dir = os.path.join(ckpt_dir, data_name)\r\n",
    "            if not os.path.exists(save_dir):\r\n",
    "                os.makedirs(save_dir)\r\n",
    "            # 保存模型参数\r\n",
    "            model.save_pretrained(save_dir)\r\n",
    "            # 保存tokenizer的词表等\r\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.4 预测和输出（评价对象级）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from model/tager/SE-ABSA16_CAME/model_state.pdparams\n",
      "fininsh predict!\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: '0', 1: '1'}\r\n",
    "# data_name = 'SE-ABSA16_CAME'     # SE-ABSA16_PHNS     SE-ABSA16_CAME\r\n",
    "test_data_loader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_seq_length, for_test=True)\r\n",
    "\r\n",
    "params_path = os.path.join(ckpt_dir, data_name + '/model_state.pdparams')\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    # 加载模型参数\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)\r\n",
    "\r\n",
    "results = predict(model, test_data_loader, label_map)\r\n",
    "\r\n",
    "# 写入预测结果\r\n",
    "save_dir = {'SE-ABSA16_PHNS': './submission/SE-ABSA16_PHNS.tsv', 'SE-ABSA16_CAME': './submission/SE-ABSA16_CAME.tsv'}\r\n",
    "res_dir = save_dir[data_name]\r\n",
    "if not os.path.exists('./submission'):\r\n",
    "    os.makedirs('./submission')\r\n",
    "# 写入预测结果\r\n",
    "with open(res_dir, 'w', encoding=\"utf8\") as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for idx, label in enumerate(results):\r\n",
    "        f.write(str(idx)+\"\\t\"+label+\"\\n\")\r\n",
    "print('fininsh predict!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 观点抽取\n",
    "观点抽取的情感分析参考\n",
    "\n",
    "Jordan的项目：[基于Skep模型的情感分析比赛](https://aistudio.baidu.com/aistudio/projectdetail/2099332)\n",
    "\n",
    "『NLP打卡营』实践课5：[情感分析预训练模型SKEP](https://aistudio.baidu.com/aistudio/projectdetail/1968542)\n",
    "\n",
    "『NLP打卡营』实践课3：[使用预训练模型实现快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    "\n",
    "\n",
    "| 数据集名称 | 训练集大小 | 开发集大小 | 测试集大小\n",
    "| -------- | -------- | -------- | -------- | \n",
    "|COTE-BD      | 8,533    |/\t\t\t|3658\n",
    "|COTE-MFW\t |41,253\t |/ \t |17,681\n",
    "|COTE-DP\t |25,258\t |/ \t |10,825\n",
    "\n",
    "\n",
    "```\n",
    "COTE-BD \n",
    "train:\n",
    "label\ttext_a\n",
    "芝罘岛\t芝罘岛骑车去过几次，它挺壮观的，毕竟是我国典型的也是最大的陆连岛咯!我喜欢去那儿，反正全岛免费咯啊哈哈哈！风景的确不错而且海水也很干净，有些地方还是军事管理，禁地来着，但是我认识军官。\n",
    "\n",
    "test:\n",
    "qid\ttext_a\n",
    "0\t毕棚沟的风景早有所闻，尤其以秋季的风景最美，但是这次去晚了，红叶全掉完了，黄叶也看不到了，下了雪只能看看雪山了，还好雪山的雄伟确实值得一看。\n",
    "```\n",
    "\n",
    "```\n",
    "COTE-MFW\n",
    "train:\n",
    "label\ttext_a\n",
    "恩施大峡谷\t秀美恩施大峡谷，因其奇、险让人流连忘返。\n",
    "\n",
    "test:\n",
    "qid\ttext_a\n",
    "0\t神女溪据说在山峡蓄水前就是条很清澈的小溪，蓄水后很多遗迹都淹没在水底了，这里的水确实和外面黄黄的水不一样。\n",
    "```\n",
    "\n",
    "```\n",
    "COTE-DP\n",
    "train:\n",
    "label\ttext_a\n",
    "重庆老灶火锅\t重庆老灶火锅还是很赞的，有机会可以尝试一下！\n",
    "\n",
    "test:\n",
    "qid\ttext_a\n",
    "0\t还是第一次进星巴克店里吃东西 那会儿第一次喝咖啡还是外带的\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1 数据提取预处理（观点抽取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '芝罘岛', 'text': '芝罘岛骑车去过几次，它挺壮观的，毕竟是我国典型的也是最大的陆连岛咯!我喜欢去那儿，反正全岛免费咯啊哈哈哈！风景的确不错而且海水也很干净，有些地方还是军事管理，禁地来着，但是我认识军官。'}\n",
      "{'qid': '0', 'text': '毕棚沟的风景早有所闻，尤其以秋季的风景最美，但是这次去晚了，红叶全掉完了，黄叶也看不到了，下了雪只能看看雪山了，还好雪山的雄伟确实值得一看。'}\n"
     ]
    }
   ],
   "source": [
    "## 加载数据，预处理\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "def read(data_path, data_type='train'):\r\n",
    "    if data_type=='train':\r\n",
    "        with open(data_path, 'r', encoding='utf8') as f:\r\n",
    "            for line in f.readlines()[1:]:\r\n",
    "                line_str_tmp = line.strip().split('\\t')\r\n",
    "                if len(line_str_tmp) == 2:\r\n",
    "                    label, text = line_str_tmp\r\n",
    "                yield {'label': label, 'text': text}\r\n",
    "    else:\r\n",
    "        with open(data_path, 'r', encoding='utf8') as f:\r\n",
    "            for line in f.readlines()[1:]:\r\n",
    "                qid, text = line.strip().split('\\t')\r\n",
    "                yield {'qid': qid, 'text': text}\r\n",
    "\r\n",
    "# 加载两个数据集的数据\r\n",
    "data_dict = {'COTE-BD':{'test': load_dataset(read, data_path='dataset/COTE-BD/test.tsv', data_type='test', lazy=False),\r\n",
    "                             'train': load_dataset(read, data_path='dataset/COTE-BD/train.tsv', data_type='train', lazy=False)},\r\n",
    "            'COTE-MFW':{'test': load_dataset(read, data_path='dataset/COTE-MFW/test.tsv', data_type='test', lazy=False),\r\n",
    "                             'train': load_dataset(read, data_path='dataset/COTE-MFW/train.tsv', data_type='train', lazy=False)},\r\n",
    "            'COTE-DP': {'test': load_dataset(read, data_path='dataset/COTE-DP/test.tsv', data_type='test', lazy=False),\r\n",
    "                           'train': load_dataset(read, data_path='dataset/COTE-DP/train.tsv', data_type='train', lazy=False)}\r\n",
    "                           }\r\n",
    "print(data_dict['COTE-BD']['train'][0])\r\n",
    "print(data_dict['COTE-BD']['test'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 准备DataLoader（观点抽取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 借鉴 【NLP打卡营】\r\n",
    "from functools import partial\r\n",
    "import paddle\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "from paddle.io import DataLoader\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "label_list = {'B': 0, 'I': 1, 'O': 2}\r\n",
    "index2label = {0: 'B', 1: 'I', 2: 'O'}\r\n",
    "no_entity_label_idx = label_list.get(\"O\", 2)\r\n",
    "\r\n",
    "## 参考\r\n",
    "def convert_example(example,\r\n",
    "                    tokenizer,\r\n",
    "                    max_seq_length=512,\r\n",
    "                    is_test=False):\r\n",
    "    text = example['text']\r\n",
    "    if is_test:\r\n",
    "        # qid = example['qid']\r\n",
    "        token_res = tokenizer.encode(text, max_seq_len=max_seq_length)\r\n",
    "        origin_enc = token_res['input_ids']\r\n",
    "        token_type_ids = token_res['token_type_ids']\r\n",
    "        # seq_len = token_res['seq_len']\r\n",
    "        return np.array(origin_enc, dtype='int64'), np.array(token_type_ids, dtype='int64')\r\n",
    "    else:\r\n",
    "        label = example['label']\r\n",
    "        # 由于并不是每个字都是一个token，这里采用一种简单的处理方法，先编码label，再编码text中除了label以外的词，最后合到一起\r\n",
    "        texts = text.split(label)\r\n",
    "        label_enc = tokenizer.encode(label)['input_ids']\r\n",
    "        cls_enc = label_enc[0]\r\n",
    "        sep_enc = label_enc[-1]\r\n",
    "        label_enc = label_enc[1:-1]\r\n",
    "        # 合并\r\n",
    "        origin_enc = []\r\n",
    "        label_ids = []\r\n",
    "        for index, text in enumerate(texts):\r\n",
    "            text_enc = tokenizer.encode(text)['input_ids']\r\n",
    "            text_enc = text_enc[1:-1]\r\n",
    "            origin_enc += text_enc\r\n",
    "            label_ids += [label_list['O']] * len(text_enc)\r\n",
    "            if index != len(texts) - 1:\r\n",
    "                origin_enc += label_enc\r\n",
    "                label_ids += [label_list['B']] + [label_list['I']] * (len(label_enc) - 1)\r\n",
    "\r\n",
    "        origin_enc = [cls_enc] + origin_enc + [sep_enc]\r\n",
    "        label_ids = [label_list['O']] + label_ids + [label_list['O']]    \r\n",
    "        # 截断\r\n",
    "        if len(origin_enc) > max_seq_length:\r\n",
    "            origin_enc = origin_enc[:max_seq_length-1] + origin_enc[-1:]\r\n",
    "            label_ids = label_ids[:max_seq_length-1] + label_ids[-1:]\r\n",
    "        \r\n",
    "        token_type_ids = [0] * len(label_ids)\r\n",
    "\r\n",
    "        return np.array(origin_enc, dtype='int64'), np.array(token_type_ids, dtype='int64'), np.array(label_ids, dtype='int64')\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data_set, tokenizer, batch_size=32, max_len=512, for_test=False):\r\n",
    "\r\n",
    "    trans_func = partial(\r\n",
    "        convert_example,\r\n",
    "        tokenizer=tokenizer,\r\n",
    "        max_seq_length=max_seq_length,\r\n",
    "        is_test=for_test)\r\n",
    "    \r\n",
    "    if for_test:\r\n",
    "        batchify_fn = lambda samples, fn=Tuple(\r\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "        ): [data for data in fn(samples)]\r\n",
    "    else:\r\n",
    "        batchify_fn = lambda samples, fn=Tuple(\r\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "            Pad(axis=0, pad_val=label_list['O'])  # labels\r\n",
    "        ): [data for data in fn(samples)]\r\n",
    "\r\n",
    "    data_set = data_set.map(trans_func)    \r\n",
    "        \r\n",
    "    # 将数据组成批量式数据，如\r\n",
    "    # 将不同长度的文本序列padding到批量式数据中最大长度\r\n",
    "    # 将每条数据label堆叠在一起\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    if for_test:\r\n",
    "        sampler = paddle.io.BatchSampler(\r\n",
    "            dataset=data_set, batch_size=batch_size, shuffle=shuffle)\r\n",
    "    else:\r\n",
    "        sampler = paddle.io.DistributedBatchSampler(\r\n",
    "            dataset=data_set, batch_size=batch_size, shuffle=shuffle)\r\n",
    "\r\n",
    "    data_loader = DataLoader(dataset=data_set, batch_sampler=sampler, collate_fn=batchify_fn)\r\n",
    "\r\n",
    "    return data_loader    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 09:11:23,412] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-24 09:11:33,860] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# 模型和分词\r\n",
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import SkepForTokenClassification, SkepTokenizer\r\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "\r\n",
    "# import paddlenlp\r\n",
    "# from paddlenlp.transformers import SkepTokenizer, SkepModel, SkepCrfForTokenClassification\r\n",
    "# skep = SkepModel.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "# model = SkepCrfForTokenClassification(\r\n",
    "#     skep, num_classes=3)\r\n",
    "# tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 训练观点提取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import time\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.metrics import Perplexity\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "\r\n",
    "# 数据\r\n",
    "data_name = 'COTE-BD'     # COTE-BD    COTE-MFW    COTE-DP\r\n",
    "\r\n",
    "# 参数\r\n",
    "paramers = {'COTE-BD': [{'batch_size': 8, 'max_seq_length': 512, \r\n",
    "                         'epochs': 1, 'learning_rate': 2e-5},\r\n",
    "                         {'batch_size': 8, 'max_seq_length': 512, \r\n",
    "                         'epochs': 4, 'learning_rate': 5e-5}],\r\n",
    "            'COTE-MFW': {'batch_size': 8, 'max_seq_length': 512, \r\n",
    "                         'epochs': 1, 'learning_rate': 5e-5},\r\n",
    "            'COTE-DP': [{'batch_size': 8, 'max_seq_length': 512, \r\n",
    "                         'epochs': 1, 'learning_rate': 5e-5},\r\n",
    "                         {'batch_size': 16, 'max_seq_length': 512, \r\n",
    "                         'epochs': 2, 'learning_rate': 2e-5}]\r\n",
    "                         }\r\n",
    "\r\n",
    "batch_size = 16   # 批量数据大小\r\n",
    "max_seq_length = 512  # 文本序列最大长度\r\n",
    "epochs = 1\r\n",
    "learning_rate = 2e-5\r\n",
    "# 训练过程中保存模型参数的文件夹\r\n",
    "ckpt_dir = \"model/point\"\r\n",
    "\r\n",
    "## 优化\r\n",
    "metric = ChunkEvaluator(label_list=label_list.keys(), suffix=True)\r\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "\r\n",
    "\r\n",
    "# print(data_dict[data_name]['train'][0])\r\n",
    "## 数据相关\r\n",
    "train_data_loader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_seq_length, for_test=False)\r\n",
    "dev_data_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 9, loss: 0.09922, speed: 2.57 step/s\n",
      "global step 20, epoch: 1, batch: 19, loss: 0.21254, speed: 2.51 step/s\n",
      "global step 30, epoch: 1, batch: 29, loss: 0.07595, speed: 2.05 step/s\n",
      "global step 40, epoch: 1, batch: 39, loss: 0.06805, speed: 2.70 step/s\n",
      "global step 50, epoch: 1, batch: 49, loss: 0.08142, speed: 2.00 step/s\n",
      "global step 60, epoch: 1, batch: 59, loss: 0.04657, speed: 2.97 step/s\n",
      "global step 70, epoch: 1, batch: 69, loss: 0.05693, speed: 2.61 step/s\n",
      "global step 80, epoch: 1, batch: 79, loss: 0.04456, speed: 2.98 step/s\n",
      "global step 90, epoch: 1, batch: 89, loss: 0.04082, speed: 2.48 step/s\n",
      "global step 100, epoch: 1, batch: 99, loss: 0.03522, speed: 2.91 step/s\n",
      "global step 110, epoch: 1, batch: 109, loss: 0.02826, speed: 0.52 step/s\n",
      "global step 120, epoch: 1, batch: 119, loss: 0.04515, speed: 2.69 step/s\n",
      "global step 130, epoch: 1, batch: 129, loss: 0.02819, speed: 2.58 step/s\n",
      "global step 140, epoch: 1, batch: 139, loss: 0.02766, speed: 2.96 step/s\n",
      "global step 150, epoch: 1, batch: 149, loss: 0.00663, speed: 2.50 step/s\n",
      "global step 160, epoch: 1, batch: 159, loss: 0.01761, speed: 2.72 step/s\n",
      "global step 170, epoch: 1, batch: 169, loss: 0.03232, speed: 2.77 step/s\n",
      "global step 180, epoch: 1, batch: 179, loss: 0.01836, speed: 2.35 step/s\n",
      "global step 190, epoch: 1, batch: 189, loss: 0.03767, speed: 2.66 step/s\n",
      "global step 200, epoch: 1, batch: 199, loss: 0.03103, speed: 2.64 step/s\n",
      "global step 210, epoch: 1, batch: 209, loss: 0.02976, speed: 0.44 step/s\n",
      "global step 220, epoch: 1, batch: 219, loss: 0.08376, speed: 2.21 step/s\n",
      "global step 230, epoch: 1, batch: 229, loss: 0.01140, speed: 3.07 step/s\n",
      "global step 240, epoch: 1, batch: 239, loss: 0.03630, speed: 3.04 step/s\n",
      "global step 250, epoch: 1, batch: 249, loss: 0.03439, speed: 2.98 step/s\n",
      "global step 260, epoch: 1, batch: 259, loss: 0.02905, speed: 2.38 step/s\n",
      "global step 270, epoch: 1, batch: 269, loss: 0.03089, speed: 2.62 step/s\n",
      "global step 280, epoch: 1, batch: 279, loss: 0.00810, speed: 2.82 step/s\n",
      "global step 290, epoch: 1, batch: 289, loss: 0.02741, speed: 2.42 step/s\n",
      "global step 300, epoch: 1, batch: 299, loss: 0.01987, speed: 2.76 step/s\n",
      "global step 310, epoch: 1, batch: 309, loss: 0.01843, speed: 0.49 step/s\n",
      "global step 320, epoch: 1, batch: 319, loss: 0.02767, speed: 2.77 step/s\n",
      "global step 330, epoch: 1, batch: 329, loss: 0.02226, speed: 2.43 step/s\n",
      "global step 340, epoch: 1, batch: 339, loss: 0.04436, speed: 2.92 step/s\n",
      "global step 350, epoch: 1, batch: 349, loss: 0.00725, speed: 1.98 step/s\n",
      "global step 360, epoch: 1, batch: 359, loss: 0.01716, speed: 2.87 step/s\n",
      "global step 370, epoch: 1, batch: 369, loss: 0.02477, speed: 2.58 step/s\n",
      "global step 380, epoch: 1, batch: 379, loss: 0.00908, speed: 2.70 step/s\n",
      "global step 390, epoch: 1, batch: 389, loss: 0.01240, speed: 2.10 step/s\n",
      "global step 400, epoch: 1, batch: 399, loss: 0.00458, speed: 2.31 step/s\n",
      "global step 410, epoch: 1, batch: 409, loss: 0.03151, speed: 0.48 step/s\n",
      "global step 420, epoch: 1, batch: 419, loss: 0.01244, speed: 2.67 step/s\n",
      "global step 430, epoch: 1, batch: 429, loss: 0.04970, speed: 2.53 step/s\n",
      "global step 440, epoch: 1, batch: 439, loss: 0.02548, speed: 2.33 step/s\n",
      "global step 450, epoch: 1, batch: 449, loss: 0.01029, speed: 2.77 step/s\n",
      "global step 460, epoch: 1, batch: 459, loss: 0.01103, speed: 2.43 step/s\n",
      "global step 470, epoch: 1, batch: 469, loss: 0.08228, speed: 2.11 step/s\n",
      "global step 480, epoch: 1, batch: 479, loss: 0.02197, speed: 2.60 step/s\n",
      "global step 490, epoch: 1, batch: 489, loss: 0.03524, speed: 2.42 step/s\n",
      "global step 500, epoch: 1, batch: 499, loss: 0.03047, speed: 3.02 step/s\n",
      "global step 510, epoch: 1, batch: 509, loss: 0.01651, speed: 0.50 step/s\n",
      "global step 520, epoch: 1, batch: 519, loss: 0.04480, speed: 2.60 step/s\n",
      "global step 530, epoch: 1, batch: 529, loss: 0.03826, speed: 2.75 step/s\n",
      "global step 540, epoch: 1, batch: 539, loss: 0.02866, speed: 2.87 step/s\n",
      "global step 550, epoch: 1, batch: 549, loss: 0.06041, speed: 2.73 step/s\n",
      "global step 560, epoch: 1, batch: 559, loss: 0.00397, speed: 2.30 step/s\n",
      "global step 570, epoch: 1, batch: 569, loss: 0.04598, speed: 2.79 step/s\n",
      "global step 580, epoch: 1, batch: 579, loss: 0.00964, speed: 2.91 step/s\n",
      "global step 590, epoch: 1, batch: 589, loss: 0.04116, speed: 2.53 step/s\n",
      "global step 600, epoch: 1, batch: 599, loss: 0.01585, speed: 1.97 step/s\n",
      "global step 610, epoch: 1, batch: 609, loss: 0.01273, speed: 0.49 step/s\n",
      "global step 620, epoch: 1, batch: 619, loss: 0.02213, speed: 2.91 step/s\n",
      "global step 630, epoch: 1, batch: 629, loss: 0.02379, speed: 2.64 step/s\n",
      "global step 640, epoch: 1, batch: 639, loss: 0.02831, speed: 2.41 step/s\n",
      "global step 650, epoch: 1, batch: 649, loss: 0.00473, speed: 2.54 step/s\n",
      "global step 660, epoch: 1, batch: 659, loss: 0.04264, speed: 2.40 step/s\n",
      "global step 670, epoch: 1, batch: 669, loss: 0.00950, speed: 2.46 step/s\n",
      "global step 680, epoch: 1, batch: 679, loss: 0.00399, speed: 2.20 step/s\n",
      "global step 690, epoch: 1, batch: 689, loss: 0.03106, speed: 2.54 step/s\n",
      "global step 700, epoch: 1, batch: 699, loss: 0.01559, speed: 2.29 step/s\n",
      "global step 710, epoch: 1, batch: 709, loss: 0.01040, speed: 0.50 step/s\n",
      "global step 720, epoch: 1, batch: 719, loss: 0.02069, speed: 2.20 step/s\n",
      "global step 730, epoch: 1, batch: 729, loss: 0.04351, speed: 2.06 step/s\n",
      "global step 740, epoch: 1, batch: 739, loss: 0.02566, speed: 2.77 step/s\n",
      "global step 750, epoch: 1, batch: 749, loss: 0.02427, speed: 2.84 step/s\n",
      "global step 760, epoch: 1, batch: 759, loss: 0.01266, speed: 2.04 step/s\n",
      "global step 770, epoch: 1, batch: 769, loss: 0.01596, speed: 2.79 step/s\n",
      "global step 780, epoch: 1, batch: 779, loss: 0.02441, speed: 2.58 step/s\n",
      "global step 790, epoch: 1, batch: 789, loss: 0.01294, speed: 2.50 step/s\n",
      "global step 800, epoch: 1, batch: 799, loss: 0.04865, speed: 1.66 step/s\n",
      "global step 810, epoch: 1, batch: 809, loss: 0.01339, speed: 0.51 step/s\n",
      "global step 820, epoch: 1, batch: 819, loss: 0.00893, speed: 2.40 step/s\n",
      "global step 830, epoch: 1, batch: 829, loss: 0.00653, speed: 1.90 step/s\n",
      "global step 840, epoch: 1, batch: 839, loss: 0.03455, speed: 2.74 step/s\n",
      "global step 850, epoch: 1, batch: 849, loss: 0.02543, speed: 2.60 step/s\n",
      "global step 860, epoch: 1, batch: 859, loss: 0.01036, speed: 1.93 step/s\n",
      "global step 870, epoch: 1, batch: 869, loss: 0.01701, speed: 3.36 step/s\n",
      "global step 880, epoch: 1, batch: 879, loss: 0.01552, speed: 3.07 step/s\n",
      "global step 890, epoch: 1, batch: 889, loss: 0.01524, speed: 2.75 step/s\n",
      "global step 900, epoch: 1, batch: 899, loss: 0.05769, speed: 3.02 step/s\n",
      "global step 910, epoch: 1, batch: 909, loss: 0.04412, speed: 0.50 step/s\n",
      "global step 920, epoch: 1, batch: 919, loss: 0.04255, speed: 2.82 step/s\n",
      "global step 930, epoch: 1, batch: 929, loss: 0.00690, speed: 2.46 step/s\n",
      "global step 940, epoch: 1, batch: 939, loss: 0.04950, speed: 3.19 step/s\n",
      "global step 950, epoch: 1, batch: 949, loss: 0.02848, speed: 1.86 step/s\n",
      "global step 960, epoch: 1, batch: 959, loss: 0.03805, speed: 2.97 step/s\n",
      "global step 970, epoch: 1, batch: 969, loss: 0.01364, speed: 2.69 step/s\n",
      "global step 980, epoch: 1, batch: 979, loss: 0.02636, speed: 3.16 step/s\n",
      "global step 990, epoch: 1, batch: 989, loss: 0.02424, speed: 2.43 step/s\n",
      "global step 1000, epoch: 1, batch: 999, loss: 0.01743, speed: 2.97 step/s\n",
      "global step 1010, epoch: 1, batch: 1009, loss: 0.00825, speed: 0.49 step/s\n",
      "global step 1020, epoch: 1, batch: 1019, loss: 0.03439, speed: 3.19 step/s\n",
      "global step 1030, epoch: 1, batch: 1029, loss: 0.01439, speed: 2.69 step/s\n",
      "global step 1040, epoch: 1, batch: 1039, loss: 0.01164, speed: 2.84 step/s\n",
      "global step 1050, epoch: 1, batch: 1049, loss: 0.02273, speed: 2.12 step/s\n",
      "global step 1060, epoch: 1, batch: 1059, loss: 0.00933, speed: 2.53 step/s\n",
      "global step 1070, epoch: 1, batch: 1069, loss: 0.04369, speed: 2.51 step/s\n",
      "global step 1080, epoch: 1, batch: 1079, loss: 0.02296, speed: 2.39 step/s\n",
      "global step 1090, epoch: 1, batch: 1089, loss: 0.02380, speed: 2.51 step/s\n",
      "global step 1100, epoch: 1, batch: 1099, loss: 0.05697, speed: 2.35 step/s\n",
      "global step 1110, epoch: 1, batch: 1109, loss: 0.00913, speed: 0.51 step/s\n",
      "global step 1120, epoch: 1, batch: 1119, loss: 0.00689, speed: 2.81 step/s\n",
      "global step 1130, epoch: 1, batch: 1129, loss: 0.01592, speed: 2.16 step/s\n",
      "global step 1140, epoch: 1, batch: 1139, loss: 0.01732, speed: 2.76 step/s\n",
      "global step 1150, epoch: 1, batch: 1149, loss: 0.00933, speed: 2.67 step/s\n",
      "global step 1160, epoch: 1, batch: 1159, loss: 0.01598, speed: 2.17 step/s\n",
      "global step 1170, epoch: 1, batch: 1169, loss: 0.00578, speed: 2.62 step/s\n",
      "global step 1180, epoch: 1, batch: 1179, loss: 0.01442, speed: 2.39 step/s\n",
      "global step 1190, epoch: 1, batch: 1189, loss: 0.01007, speed: 2.55 step/s\n",
      "global step 1200, epoch: 1, batch: 1199, loss: 0.00906, speed: 2.95 step/s\n",
      "global step 1210, epoch: 1, batch: 1209, loss: 0.01471, speed: 0.49 step/s\n",
      "global step 1220, epoch: 1, batch: 1219, loss: 0.04140, speed: 2.14 step/s\n",
      "global step 1230, epoch: 1, batch: 1229, loss: 0.01330, speed: 2.32 step/s\n",
      "global step 1240, epoch: 1, batch: 1239, loss: 0.02527, speed: 2.30 step/s\n",
      "global step 1250, epoch: 1, batch: 1249, loss: 0.01021, speed: 2.70 step/s\n",
      "global step 1260, epoch: 1, batch: 1259, loss: 0.01515, speed: 2.62 step/s\n",
      "global step 1270, epoch: 1, batch: 1269, loss: 0.01509, speed: 2.70 step/s\n",
      "global step 1280, epoch: 1, batch: 1279, loss: 0.01034, speed: 2.69 step/s\n",
      "global step 1290, epoch: 1, batch: 1289, loss: 0.01280, speed: 2.46 step/s\n",
      "global step 1300, epoch: 1, batch: 1299, loss: 0.02392, speed: 1.84 step/s\n",
      "global step 1310, epoch: 1, batch: 1309, loss: 0.01810, speed: 0.51 step/s\n",
      "global step 1320, epoch: 1, batch: 1319, loss: 0.01383, speed: 2.09 step/s\n",
      "global step 1330, epoch: 1, batch: 1329, loss: 0.00301, speed: 2.39 step/s\n",
      "global step 1340, epoch: 1, batch: 1339, loss: 0.01586, speed: 2.27 step/s\n",
      "global step 1350, epoch: 1, batch: 1349, loss: 0.01120, speed: 2.78 step/s\n",
      "global step 1360, epoch: 1, batch: 1359, loss: 0.01430, speed: 2.74 step/s\n",
      "global step 1370, epoch: 1, batch: 1369, loss: 0.02999, speed: 2.70 step/s\n",
      "global step 1380, epoch: 1, batch: 1379, loss: 0.01225, speed: 2.29 step/s\n",
      "global step 1390, epoch: 1, batch: 1389, loss: 0.02744, speed: 2.74 step/s\n",
      "global step 1400, epoch: 1, batch: 1399, loss: 0.01120, speed: 2.36 step/s\n",
      "global step 1410, epoch: 1, batch: 1409, loss: 0.03665, speed: 0.48 step/s\n",
      "global step 1420, epoch: 1, batch: 1419, loss: 0.01880, speed: 2.31 step/s\n",
      "global step 1430, epoch: 1, batch: 1429, loss: 0.02284, speed: 2.41 step/s\n",
      "global step 1440, epoch: 1, batch: 1439, loss: 0.01106, speed: 2.53 step/s\n",
      "global step 1450, epoch: 1, batch: 1449, loss: 0.01905, speed: 2.48 step/s\n",
      "global step 1460, epoch: 1, batch: 1459, loss: 0.04093, speed: 2.34 step/s\n",
      "global step 1470, epoch: 1, batch: 1469, loss: 0.00969, speed: 2.31 step/s\n",
      "global step 1480, epoch: 1, batch: 1479, loss: 0.03778, speed: 2.03 step/s\n",
      "global step 1490, epoch: 1, batch: 1489, loss: 0.01953, speed: 2.46 step/s\n",
      "global step 1500, epoch: 1, batch: 1499, loss: 0.00255, speed: 2.01 step/s\n",
      "global step 1510, epoch: 1, batch: 1509, loss: 0.00728, speed: 0.50 step/s\n",
      "global step 1520, epoch: 1, batch: 1519, loss: 0.01530, speed: 2.32 step/s\n",
      "global step 1530, epoch: 1, batch: 1529, loss: 0.01760, speed: 2.01 step/s\n",
      "global step 1540, epoch: 1, batch: 1539, loss: 0.01062, speed: 2.06 step/s\n",
      "global step 1550, epoch: 1, batch: 1549, loss: 0.02957, speed: 2.60 step/s\n",
      "global step 1560, epoch: 1, batch: 1559, loss: 0.01069, speed: 2.91 step/s\n",
      "global step 1570, epoch: 1, batch: 1569, loss: 0.01063, speed: 2.47 step/s\n",
      "global step 1580, epoch: 1, batch: 1579, loss: 0.00427, speed: 2.78 step/s\n",
      "global step 1590, epoch: 1, batch: 1589, loss: 0.01863, speed: 2.24 step/s\n",
      "global step 1600, epoch: 1, batch: 1599, loss: 0.01036, speed: 2.70 step/s\n",
      "global step 1610, epoch: 1, batch: 1609, loss: 0.00860, speed: 0.50 step/s\n",
      "global step 1620, epoch: 1, batch: 1619, loss: 0.01187, speed: 2.85 step/s\n",
      "global step 1630, epoch: 1, batch: 1629, loss: 0.01577, speed: 2.07 step/s\n",
      "global step 1640, epoch: 1, batch: 1639, loss: 0.03592, speed: 2.96 step/s\n",
      "global step 1650, epoch: 1, batch: 1649, loss: 0.00818, speed: 2.58 step/s\n",
      "global step 1660, epoch: 1, batch: 1659, loss: 0.01638, speed: 2.40 step/s\n",
      "global step 1670, epoch: 1, batch: 1669, loss: 0.02010, speed: 2.35 step/s\n",
      "global step 1680, epoch: 1, batch: 1679, loss: 0.01153, speed: 2.29 step/s\n",
      "global step 1690, epoch: 1, batch: 1689, loss: 0.01555, speed: 2.16 step/s\n",
      "global step 1700, epoch: 1, batch: 1699, loss: 0.02449, speed: 2.23 step/s\n",
      "global step 1710, epoch: 1, batch: 1709, loss: 0.00959, speed: 0.49 step/s\n",
      "global step 1720, epoch: 1, batch: 1719, loss: 0.01211, speed: 2.50 step/s\n",
      "global step 1730, epoch: 1, batch: 1729, loss: 0.01239, speed: 2.71 step/s\n",
      "global step 1740, epoch: 1, batch: 1739, loss: 0.02174, speed: 2.14 step/s\n",
      "global step 1750, epoch: 1, batch: 1749, loss: 0.01989, speed: 3.09 step/s\n",
      "global step 1760, epoch: 1, batch: 1759, loss: 0.01796, speed: 2.49 step/s\n",
      "global step 1770, epoch: 1, batch: 1769, loss: 0.01343, speed: 2.63 step/s\n",
      "global step 1780, epoch: 1, batch: 1779, loss: 0.01563, speed: 2.48 step/s\n",
      "global step 1790, epoch: 1, batch: 1789, loss: 0.00864, speed: 2.47 step/s\n",
      "global step 1800, epoch: 1, batch: 1799, loss: 0.01323, speed: 1.63 step/s\n",
      "global step 1810, epoch: 1, batch: 1809, loss: 0.00898, speed: 0.52 step/s\n",
      "global step 1820, epoch: 1, batch: 1819, loss: 0.02050, speed: 2.50 step/s\n",
      "global step 1830, epoch: 1, batch: 1829, loss: 0.03580, speed: 2.30 step/s\n",
      "global step 1840, epoch: 1, batch: 1839, loss: 0.04155, speed: 2.94 step/s\n",
      "global step 1850, epoch: 1, batch: 1849, loss: 0.01458, speed: 2.78 step/s\n",
      "global step 1860, epoch: 1, batch: 1859, loss: 0.00350, speed: 2.52 step/s\n",
      "global step 1870, epoch: 1, batch: 1869, loss: 0.01142, speed: 2.50 step/s\n",
      "global step 1880, epoch: 1, batch: 1879, loss: 0.00501, speed: 2.75 step/s\n",
      "global step 1890, epoch: 1, batch: 1889, loss: 0.01093, speed: 2.46 step/s\n",
      "global step 1900, epoch: 1, batch: 1899, loss: 0.02378, speed: 2.34 step/s\n",
      "global step 1910, epoch: 1, batch: 1909, loss: 0.01362, speed: 0.51 step/s\n",
      "global step 1920, epoch: 1, batch: 1919, loss: 0.02382, speed: 2.74 step/s\n",
      "global step 1930, epoch: 1, batch: 1929, loss: 0.00914, speed: 2.81 step/s\n",
      "global step 1940, epoch: 1, batch: 1939, loss: 0.03981, speed: 2.06 step/s\n",
      "global step 1950, epoch: 1, batch: 1949, loss: 0.00181, speed: 2.25 step/s\n",
      "global step 1960, epoch: 1, batch: 1959, loss: 0.02297, speed: 2.23 step/s\n",
      "global step 1970, epoch: 1, batch: 1969, loss: 0.02443, speed: 2.22 step/s\n",
      "global step 1980, epoch: 1, batch: 1979, loss: 0.00608, speed: 2.79 step/s\n",
      "global step 1990, epoch: 1, batch: 1989, loss: 0.00438, speed: 2.80 step/s\n",
      "global step 2000, epoch: 1, batch: 1999, loss: 0.00427, speed: 2.45 step/s\n",
      "global step 2010, epoch: 1, batch: 2009, loss: 0.01839, speed: 0.51 step/s\n",
      "global step 2020, epoch: 1, batch: 2019, loss: 0.02406, speed: 2.26 step/s\n",
      "global step 2030, epoch: 1, batch: 2029, loss: 0.01898, speed: 2.38 step/s\n",
      "global step 2040, epoch: 1, batch: 2039, loss: 0.01396, speed: 2.85 step/s\n",
      "global step 2050, epoch: 1, batch: 2049, loss: 0.04233, speed: 2.53 step/s\n",
      "global step 2060, epoch: 1, batch: 2059, loss: 0.00743, speed: 2.58 step/s\n",
      "global step 2070, epoch: 1, batch: 2069, loss: 0.05762, speed: 2.41 step/s\n",
      "global step 2080, epoch: 1, batch: 2079, loss: 0.01072, speed: 2.84 step/s\n",
      "global step 2090, epoch: 1, batch: 2089, loss: 0.02330, speed: 2.61 step/s\n",
      "global step 2100, epoch: 1, batch: 2099, loss: 0.01068, speed: 1.86 step/s\n",
      "global step 2110, epoch: 1, batch: 2109, loss: 0.00534, speed: 0.51 step/s\n",
      "global step 2120, epoch: 1, batch: 2119, loss: 0.01245, speed: 2.45 step/s\n",
      "global step 2130, epoch: 1, batch: 2129, loss: 0.00643, speed: 2.55 step/s\n",
      "global step 2140, epoch: 1, batch: 2139, loss: 0.02424, speed: 2.55 step/s\n",
      "global step 2150, epoch: 1, batch: 2149, loss: 0.01025, speed: 2.73 step/s\n",
      "global step 2160, epoch: 1, batch: 2159, loss: 0.02778, speed: 2.70 step/s\n",
      "global step 2170, epoch: 1, batch: 2169, loss: 0.00809, speed: 2.41 step/s\n",
      "global step 2180, epoch: 1, batch: 2179, loss: 0.00324, speed: 2.65 step/s\n",
      "global step 2190, epoch: 1, batch: 2189, loss: 0.01551, speed: 2.76 step/s\n",
      "global step 2200, epoch: 1, batch: 2199, loss: 0.01563, speed: 2.22 step/s\n",
      "global step 2210, epoch: 1, batch: 2209, loss: 0.00743, speed: 0.50 step/s\n",
      "global step 2220, epoch: 1, batch: 2219, loss: 0.01177, speed: 2.35 step/s\n",
      "global step 2230, epoch: 1, batch: 2229, loss: 0.00494, speed: 2.55 step/s\n",
      "global step 2240, epoch: 1, batch: 2239, loss: 0.01066, speed: 2.10 step/s\n",
      "global step 2250, epoch: 1, batch: 2249, loss: 0.01271, speed: 2.40 step/s\n",
      "global step 2260, epoch: 1, batch: 2259, loss: 0.00941, speed: 2.86 step/s\n",
      "global step 2270, epoch: 1, batch: 2269, loss: 0.03294, speed: 2.57 step/s\n",
      "global step 2280, epoch: 1, batch: 2279, loss: 0.01464, speed: 2.22 step/s\n",
      "global step 2290, epoch: 1, batch: 2289, loss: 0.01480, speed: 2.25 step/s\n",
      "global step 2300, epoch: 1, batch: 2299, loss: 0.01457, speed: 2.67 step/s\n",
      "global step 2310, epoch: 1, batch: 2309, loss: 0.00879, speed: 0.51 step/s\n",
      "global step 2320, epoch: 1, batch: 2319, loss: 0.00951, speed: 3.31 step/s\n",
      "global step 2330, epoch: 1, batch: 2329, loss: 0.00912, speed: 2.48 step/s\n",
      "global step 2340, epoch: 1, batch: 2339, loss: 0.01617, speed: 2.31 step/s\n",
      "global step 2350, epoch: 1, batch: 2349, loss: 0.01435, speed: 2.82 step/s\n",
      "global step 2360, epoch: 1, batch: 2359, loss: 0.02758, speed: 2.24 step/s\n",
      "global step 2370, epoch: 1, batch: 2369, loss: 0.01754, speed: 2.11 step/s\n",
      "global step 2380, epoch: 1, batch: 2379, loss: 0.01336, speed: 2.44 step/s\n",
      "global step 2390, epoch: 1, batch: 2389, loss: 0.01447, speed: 2.64 step/s\n",
      "global step 2400, epoch: 1, batch: 2399, loss: 0.00647, speed: 2.18 step/s\n",
      "global step 2410, epoch: 1, batch: 2409, loss: 0.00934, speed: 0.50 step/s\n",
      "global step 2420, epoch: 1, batch: 2419, loss: 0.05455, speed: 2.92 step/s\n",
      "global step 2430, epoch: 1, batch: 2429, loss: 0.01581, speed: 2.89 step/s\n",
      "global step 2440, epoch: 1, batch: 2439, loss: 0.01535, speed: 2.50 step/s\n",
      "global step 2450, epoch: 1, batch: 2449, loss: 0.02265, speed: 2.89 step/s\n",
      "global step 2460, epoch: 1, batch: 2459, loss: 0.00926, speed: 2.28 step/s\n",
      "global step 2470, epoch: 1, batch: 2469, loss: 0.01569, speed: 2.45 step/s\n",
      "global step 2480, epoch: 1, batch: 2479, loss: 0.01340, speed: 2.62 step/s\n",
      "global step 2490, epoch: 1, batch: 2489, loss: 0.01913, speed: 2.81 step/s\n",
      "global step 2500, epoch: 1, batch: 2499, loss: 0.01411, speed: 2.50 step/s\n",
      "global step 2510, epoch: 1, batch: 2509, loss: 0.01585, speed: 0.51 step/s\n",
      "global step 2520, epoch: 1, batch: 2519, loss: 0.00843, speed: 2.81 step/s\n",
      "global step 2530, epoch: 1, batch: 2529, loss: 0.00430, speed: 2.19 step/s\n",
      "global step 2540, epoch: 1, batch: 2539, loss: 0.02296, speed: 2.51 step/s\n",
      "global step 2550, epoch: 1, batch: 2549, loss: 0.01618, speed: 2.56 step/s\n",
      "global step 2560, epoch: 1, batch: 2559, loss: 0.03012, speed: 2.45 step/s\n",
      "global step 2570, epoch: 1, batch: 2569, loss: 0.03187, speed: 2.85 step/s\n"
     ]
    }
   ],
   "source": [
    "global_step = 0 \r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, (input_ids, token_type_ids, labels) in enumerate(train_data_loader):\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\r\n",
    "\r\n",
    "    \r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0:\r\n",
    "            print(\r\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss,\r\n",
    "                    10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "\r\n",
    "        # 反向梯度回传，更新参数\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % 100 == 0:\r\n",
    "            save_dir = os.path.join(ckpt_dir, data_name)\r\n",
    "            if not os.path.exists(save_dir):\r\n",
    "                os.makedirs(save_dir)\r\n",
    "            # 保存模型参数\r\n",
    "            model.save_pretrained(save_dir)\r\n",
    "            # 保存tokenizer的词表等\r\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.4 预测和输出（观点提取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_name = 'COTE-BD'     # COTE-BD    COTE-MFW    COTE-DP\r\n",
    "test_data_loader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_seq_length, for_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from model/point/COTE-BD/model_state.pdparams\n",
      "fininsh predict!\n"
     ]
    }
   ],
   "source": [
    "params_path = os.path.join(ckpt_dir, data_name + '/model_state.pdparams')\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    # 加载模型参数\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)\r\n",
    "\r\n",
    "\r\n",
    "@paddle.no_grad()\r\n",
    "def predict(model, data_loader):\r\n",
    "    model.eval()\r\n",
    "    pred_list = []\r\n",
    "    input_ids_list = []\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, token_type_ids = batch\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        pred = paddle.argmax(logits, axis=-1).numpy()\r\n",
    "        pred = pred.tolist()\r\n",
    "        input_ids = input_ids.numpy().tolist()\r\n",
    "        pred_list.extend(pred)\r\n",
    "        input_ids_list.extend(input_ids)\r\n",
    "\r\n",
    "    return pred_list, input_ids_list\r\n",
    "\r\n",
    "\r\n",
    "predicts, input_ids_list = predict(model, test_data_loader)\r\n",
    "\r\n",
    "def find_entity(prediction, input_ids):\r\n",
    "    entity = []\r\n",
    "    entity_ids = []\r\n",
    "    for index, idx in enumerate(prediction):\r\n",
    "        if idx == label_list['B']:\r\n",
    "            entity_ids = [input_ids[index]]\r\n",
    "        elif idx == label_list['I']:\r\n",
    "            if entity_ids:\r\n",
    "                entity_ids.append(input_ids[index])\r\n",
    "        elif idx == label_list['O']:\r\n",
    "            if entity_ids:\r\n",
    "                entity.append(''.join(tokenizer.convert_ids_to_tokens(entity_ids)))\r\n",
    "                entity_ids = []\r\n",
    "    return entity\r\n",
    "\r\n",
    "import re\r\n",
    "# 写入预测结果\r\n",
    "save_dir = {'COTE-BD': './submission/COTE_BD.tsv', 'COTE-MFW': './submission/COTE_MFW.tsv', 'COTE-DP': './submission/COTE_DP.tsv'}\r\n",
    "res_dir = save_dir[data_name]\r\n",
    "if not os.path.exists('./submission'):\r\n",
    "    os.makedirs('./submission')\r\n",
    "# 写入预测结果\r\n",
    "with open(res_dir, 'w', encoding=\"utf8\") as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for idx, prediction in enumerate(predicts):\r\n",
    "        entity = find_entity(prediction, input_ids_list[idx])\r\n",
    "        entity = list(set(entity))  # 去重\r\n",
    "        entity = [re.sub('##', '', e) for e in entity]  # 去除英文编码时的特殊符号\r\n",
    "        entity = [re.sub('[UNK]', '', e) for e in entity]  # 去除未知符号\r\n",
    "        entity = [re.sub('\\\"', '', e) for e in entity]  # 去除引号\r\n",
    "        f.write(str(idx) + '\\t' + '\\x01'.join(entity) + '\\n')\r\n",
    "print('fininsh predict!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/submission\n",
      "  adding: COTE_BD.tsv (deflated 44%)\n",
      "  adding: COTE_DP.tsv (deflated 54%)\n",
      "  adding: COTE_MFW.tsv (deflated 54%)\n",
      "  adding: ChnSentiCorp.tsv (deflated 63%)\n",
      "  adding: NLPCC14-SC.tsv (deflated 64%)\n",
      "  adding: SE-ABSA16_CAME.tsv (deflated 64%)\n",
      "  adding: SE-ABSA16_PHNS.tsv (deflated 64%)\n"
     ]
    }
   ],
   "source": [
    "%cd submission\r\n",
    "!zip -r ../submission.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "\n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐，及时跟踪最新消息和功能哦**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
